---
title: 深度学习数学基础-大数定律和中心极限定律
tags:
  - DeepLearning
  - Probability
categories:
  - AI
  - Mathematics
toc: true
cover: '/assets/images/20190824221538.webp'
abbrlink: 1a21e3ee
date: 2019-08-24 22:12:28
---

# 大数定律

> 大数定律一般有以下几种表述方式

<!-- more -->

## 切比雪夫($Chebyshev$)大数定律（一般形式）

设 $X_1,X_2,\dots, X_n, \dots$ 是由两两不相关（或独立）的随机事件组成的序列，分别具有数学期望：$E(X_1),E(X_2),\dots,E(X_n)
,\dots$ 和方差$D(X_1),D(X_2),\dots,D(X_n),\dots$，**且方差具有公共上界**，即：存在正数$M\gt0$，使得：$|D(X_i)| \lt M,
i=1,2,3,\dots$，则对于任意给定的正数$\varepsilon \gt 0$有以下结论：
$$
\lim_{n \to \infty} P \left\lbrace \left| \frac{1}{n} \sum_{k=1}^nX_k - \frac{1}{n} \sum_{k=1}^n E(X_k)\right| \lt
\varepsilon \right\rbrace = 1
\tag{1}
$$

> 不相关：
>
> ​ 不是正相关或者是负相关，$X_1，X_2$不相关也就是，二者之间没有相互影响的变化，可能在微观上并不是独立的，但是在宏观层面上，其中一个事件并不会影响到另一个事件。

在$(1)$式中，$\frac{1}{n} \sum_{k=1}^nX_k$是变量的均值，$\frac{1}{n} \sum_{k=1}^n E(X_k)$是变量期望的均值，用一句话来通俗的解释就是：

>
当数量足够大时，独立随机变量的均值，与其期望的均值在数值上是相同的。（也可以说变量的均值收敛于期望的均值，或：样本均值[依概率收敛](https://zh.wikipedia.org/wiki/依概率收敛)
于期望值）

## 独立同分布的切比雪夫大数定律（特殊形式）

设 $X_1,X_2,\dots, X_n, \dots$ 是由相互独立，且服从相同分布，具有数学期望$E(X_n) = \mu$和方差$D(X_n)
=\sigma^2$，其中$n=1,2,3,\dots$，则对于任意给定的正数$\varepsilon \gt 0$有以下结论：
$$
\lim_{n \to \infty} P \left\lbrace \left| \frac{1}{n} \sum_{k=1}^nX_k - \mu \right| \lt \varepsilon \right\rbrace = 1
\tag{2}
$$
$(2)$式的解释其实与(1)式是一致的。

## 辛钦($Wiener-khinchin$)大数定律

设 $X_1,X_2,\dots, X_n, \dots$ 是由相互独立，且服从相同分布，具有数学期望$E(X_n) =
\mu$和，其中$n=1,2,3,\dots$，则对于任意给定的正数$\varepsilon \gt 0$有以下结论：
$$
\lim_{n \to \infty} P \left\lbrace \left| \frac{1}{n} \sum_{k=1}^nX_k - \mu \right| \lt \varepsilon \right\rbrace = 1
\tag{2}
$$
**辛钦大数定律**与**比雪夫大数定律的特殊形式**的差别在于，不再规定方差是否相同，其解释为：

> 不管数据是否离散，或离散程度的大小，最终，其均值都将收敛于期望。

## 伯努利大数定律

设在每次实验中，时间$A$发生的概率$P(A) = p$，在$n$次独立重复试验中，事件$A$发生的频率为$f_n(A)
$，则对于任意正数$\varepsilon$，总有:
$$
\lim_{n \to \infty} P \left\lbrace \left | f_n(A) -p \right | \lt \varepsilon \right \rbrace = 1
\tag{3}
$$

> ~~这个有点类似于**墨菲定律**，就是，概率不是$0$的事件，总会发生，只要重复的次数够多。~~
>
> 理解错了，伯努利大数定律的意义在于，只有重复的次数多，才能反映真实的概率。
>
>
比如，硬币的两面概率分别为$50\%$,但是，你只实验100次的话，可能会发生48次正面，52次反面的情况，计算出的频率为正面$\frac{48}{100} =
48\%$，反面$\frac{52}{100} = 52\%$。
>
> 但是按照伯努利大数定律的情况下，要试验的次数够多，正面和反面的频率最终会收敛至真实概率。
>
> > 个例并不能代表整体真实概率。

# 中心极限定律

## 棣($d\grave{i}$)莫佛-拉普拉斯($de\ Moivre - Laplace$)中心极限定律

设随机变量$X_n$服从参数为$n$和$p$的二项分布，即$X_n \sim B(n,p),\ (0 \lt p \lt 1, n=1,2,\dots )$，则对于任意的实数$x$， 有：
$$
\lim_{n \to \infty} P\left \lbrace \frac{X_n - np}{\sqrt{np(1-p)}} \le x \right \rbrace = \Phi(x)
\tag{4}
$$


> 二项分布又叫做伯努利分布$Binomial\ distribution$,其中的参数：
>
> $n$： 独立实验的次数。
>
> $p$：每次实验的概率。

> 其中的$\Phi(x)$是一个正态分布。

> 另外：包括泊松分布$P(\lambda)$，卡方分布$\chi^2$，T分$t$, 分别在$\lambda、n、t$很大的时候都接近于正态分布。

## 列维-林德伯格中心极限定律

设随机变量$X_1,X_2,\dots,X_n,\dots$相互独立，服从相同分布，具有数学期望$E(X_n) = \mu$和方差$D(X_n) =
\sigma^2$，其中$n=1,2,\dots$，则对于任意实数$x$，有如下结论：
$$
\lim_{n \to \infty} P \left \lbrace \frac{\sum_{k=1}^n X_k-n\mu}{\sqrt{n} \sigma } \le x \right \rbrace = \Phi(x)
\tag{5}
$$

> 样本够多的话，随机采样会生成正太分布。

## 结语

> 中心极限定律中的中心两个字，表示的是，该极限定律在概率与统计中的地位，用于说明其重要性，而不是定律本身和中心有什么关系，

更多资料：

> [维基百科-大数定律](https://zh.wikipedia.org/wiki/%E5%A4%A7%E6%95%B0%E5%AE%9A%E5%BE%8B)
>
> [维基百科-中心极限定律](https://zh.wikipedia.org/wiki/中心极限定理)
