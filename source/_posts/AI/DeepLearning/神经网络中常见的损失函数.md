---
title: 神经网络中常见的损失函数
tags:
  - 数学
  - 深度学习
categories:
  - AI
  - DeepLearning
toc: true
cover: /assets/images/flower-meadow-4999277__340.webp
abbrlink: c728e95f
date: 2020-04-17T18:01:12.000Z
thumbnail: /assets/thumbnail/flower-meadow-4999277__340.webp
---

**损失函数**用来评价模型的**预测值**和**真实值**不一样的程度，损失函数越好，通常模型的性能越好。不同的模型用的损失函数一般也不一样。

**损失函数**分为**经验风险损失函数**和**结构风险损失函数**
。经验风险损失函数指预测结果和实际结果的差别，结构风险损失函数是指经验风险损失函数加上正则项。一般用于防止过拟合，模型越复杂，其正则项的值就越大，相应的结构化风险损失函数的值就越大，相应的损失就越大。

常见的损失函数以及其优缺点如下：

<!-- more -->

# 0-1 损失函数(Zero-One Loss)

`0-1`损失是指预测值和目标值不相等为`1`， 否则为`0`:

$$
L ( Y , f ( X ) ) = \left \lbrace
\begin{align}
1 , & \qquad Y \neq f ( X )  \\\\
0 , & \qquad Y = f ( X )
\end{align}
\right.
$$

特点：

(1)0-1 损失函数直接对应分类判断错误的个数，但是它是一个非凸函数，不太适用.

(2)**感知机**就是用的这种损失函数。但是相等这个条件太过严格，因此可以放宽条件，即满足$|Y - f(x)| < T$时认为相等，

$$
L ( Y , f ( X ) ) = \left \lbrace
\begin{align}
1 , & \qquad | Y - f ( X ) | \ge T \\\\
0 , & \qquad| Y - f ( X ) | < T
\end{align}
\right.
$$

# 绝对值损失函数

绝对值损失函数是计算预测值与目标值的差的绝对值：

$$
L(Y, f(x)) = |Y - f(x)|
$$

# log 对数损失函数

**log 对数损失函数**的标准形式如下：

$$
L(Y, P(Y|X)) = -\log P(Y|X)
$$

特点：

1. log 对数损失函数能非常好的表征概率分布，在很多场景尤其是多分类，如果需要知道结果属于每个类别的置信度，那它非常适合。
2. 健壮性不强，相比于 hinge loss 对噪声更敏感。
3. **逻辑回归**的损失函数就是 log 对数损失函数。

# 平方损失函数

**平方损失函数**标准形式如下，经常应用与回归问题。

$$
L ( Y | f ( X ) ) = \sum _ { N } ( Y - f ( X ) ) ^ { 2 }
$$

# 指数损失函数（exponential loss）

**指数损失函数**的标准形式如下,对离群点、噪声非常敏感。经常用在 AdaBoost 算法中。

$$
L(Y|f(X)) = \exp[-yf(x)]
$$

# Hinge 损失函数

Hinge 损失函数标准形式如下：

$$
L(y, f(x)) = \max(0, 1-yf(x))
$$

特点：

1. hinge 损失函数表示如果被分类正确，损失为 0，否则损失就为$1-yf(x)$ 。`SVM`就是使用这个损失函数。
2. 一般的 [公式] 是预测值，在-1 到 1 之间，$y$ 是目标值(-1 或 1)。其含义是，$f(x)$的值在-1 和+1 之间就可以了，并不鼓励 $|f(
   x)| > 1$，即并不鼓励分类器过度自信，让某个正确分类的样本距离分割线超过 1 并不会有任何奖励，从而使分类器可以更专注于整体的误差。
3. 健壮性相对较高，对异常点、噪声不敏感，但它没太好的概率解释。

# 感知损失(perceptron loss)函数

感知损失函数的标准形式如下：

$$
L(y, f(x)) = max(0, -f(x))
$$

特点：

是 Hinge 损失函数的一个变种，Hinge loss 对判定边界附近的点(正确端)惩罚力度很高。而 perceptron loss
只要样本的判定类别正确的话，它就满意，不管其判定边界的距离。它比 Hinge loss 简单，因为不是 max-margin boundary，所以模型的泛化能力没
hinge loss 强。

# 交叉熵损失函数 (Cross-entropy loss function)

**交叉熵损失函数**的标准形式如下**:**

$$
C = - \frac { 1 } { n } \sum _ { x } [ y \ln a + ( 1 - y ) \ln ( 1 - a ) ]
$$

注意公式中$x$表示样本， $y$表示实际的标签，$a$表示预测的输出，$n$表示样本总数量。

特点：

1. 本质上也是一种对数似然函数，可用于二分类和多分类任务中。

- 二分类问题中的 loss 函数（输入数据是 softmax 或者 sigmoid 函数的输出）：

$$
loss = - \frac { 1 } { n } \sum _ { x } [ y \ln a + ( 1 - y ) \ln ( 1 - a ) ]
$$

- 多分类问题中的 loss 函数（输入数据是 softmax 或者 sigmoid 函数的输出）：

$$
loss = - \frac{1}{n} \sum_i y_i \ln a_i
$$

2. 当使用 sigmoid 作为激活函数的时候，常用交叉熵损失函数而不用均方误差损失函数，因为它可以完美解决平方损失函数权重更新过慢的问题，具有“误差大的时候，权重更新快；误差小的时候，权重更新慢”的良好性质。
